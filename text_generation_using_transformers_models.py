# -*- coding: utf-8 -*-
"""Text_generation_using_transformers_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DvazEaXKQ_BzBvVp0h646jF6m5_LocC0

### Project: Text Generation using Transformer Models
This project explores how Transformer-based models like GPT-2, DistilGPT2, and T5 can generate coherent and creative text. It includes training a small model on custom data, fine-tuning on a specific domain (e.g., poetry, code, or stories), and generating responses based on prompts.

We'll use Hugging Face Transformers for implementation.
"""

!pip install transformers datasets --quiet
!pip install accelerate --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TrainingArguments, Trainer
import torch

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = "Once upon a time"
generated = text_generator(prompt, max_length=50, do_sample=True, temperature=0.7)
print(generated[0]['generated_text'])

prompt = "Explain quantum physics to a 10-year-old:"
output = text_generator(prompt, max_length=100, temperature=0.8, top_k=50, top_p=0.95)
print(output[0]['generated_text'])

import math
model.eval()

def calculate_perplexity(text):
    encodings = tokenizer(text, return_tensors="pt")
    input_ids = encodings.input_ids
    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss
    return math.exp(loss.item())

test_text = "The sun rises in the east."
print("Perplexity:", calculate_perplexity(test_text))

"""#Conclusion
 We demonstrated text generation using multiple transformer models. Fine-tuning improves coherence in domain-specific applications. This project highlights how large language models can be customized for creative and useful NLP tasks.
"""